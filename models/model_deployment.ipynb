{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Model Deployment to Vertex AI\n",
    "\n",
    "This notebook guides you through deploying a Hugging Face model to Vertex AI. It covers:\n",
    "1. Setting up your environment\n",
    "2. Downloading and packaging your model\n",
    "3. Building and pushing the Docker container\n",
    "4. Deploying to Vertex AI\n",
    "\n",
    "It uses environment variables that should be set from your infrastructure configuration provivded under `models/config/`.\n",
    "\n",
    "Required environment variables:\n",
    "```\n",
    "GCP_PROJECT_ID            # Your GCP project ID\n",
    "GCP_REGION               # Your GCP region\n",
    "GCP_ARTIFACT_REGISTRY    # Artifact Registry repository\n",
    "MODEL_ARTIFACTS_BUCKET   # GCS bucket for model artifacts\n",
    "VERTEX_AI_ENDPOINT       # Vertex AI endpoint name\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- Google Cloud SDK installed and configured\n",
    "- Docker installed and running\n",
    "- Required Python packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-cloud-aiplatform huggingface-hub transformers torch google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Set the project root path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root detected at: /home/steffen/sign-language-translator\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict\n",
    "import sys\n",
    "\n",
    "def setup_project_path():\n",
    "    \"\"\"Add project root to Python path by searching for .git directory\"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    \n",
    "    # Search up the directory tree for .git folder or pyproject.toml\n",
    "    root_indicators = ['.git', 'pyproject.toml']\n",
    "    \n",
    "    while current_path != current_path.parent:\n",
    "        if any((current_path / indicator).exists() for indicator in root_indicators):\n",
    "            sys.path.append(str(current_path))\n",
    "            return current_path\n",
    "        current_path = current_path.parent\n",
    "    \n",
    "    raise RuntimeError(\n",
    "        \"Could not find project root. \"\n",
    "        \"Please run this notebook from within the project directory.\"\n",
    "    )\n",
    "\n",
    "# Setup path\n",
    "project_root = setup_project_path()\n",
    "print(f\"Project root detected at: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your project configuration and model details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from .env\n",
      "Loading configuration from /home/steffen/sign-language-translator/models/vertex_ai/config/dev.yaml\n",
      "Configuration loaded successfully!\n",
      "\n",
      "Environment settings:\n",
      "{\n",
      "  \"environment\": \"dev\",\n",
      "  \"project_id\": \"sign-lang-translator-20241029\",\n",
      "  \"region\": \"europe-west3\",\n",
      "  \"artifact_registry_repo\": \"dev-vertex-ai-repo\",\n",
      "  \"artifacts_bucket\": \"sign-lang-translator-20241029-dev-vertex-ai-artifacts\",\n",
      "  \"endpoint_name\": \"vertex-ai\",\n",
      "  \"model_id\": \"openai/whisper-small\",\n",
      "  \"image_name\": \"untrained-predictor\",\n",
      "  \"model_version\": \"v1\",\n",
      "  \"hf_task\": \"text-to-speech\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict\n",
    "from models.vertex_ai import get_config\n",
    "\n",
    "# Model-specific configuration that might change between deployments\n",
    "model_config = {\n",
    "    'model_id': 'openai/whisper-small',  # Hugging Face model ID\n",
    "    'image_name': 'untrained-predictor',           # Docker image name\n",
    "    'model_version': 'v1',                  # Model version\n",
    "    'hf_task': 'text-to-speech',  # Hugging Face task type\n",
    "}\n",
    "\n",
    "# Load environment configuration\n",
    "try:\n",
    "    vertex_ai_config = get_config(\"dev\") # Get vertex ai configuration for 'dev' environment\n",
    "    \n",
    "    # Access config properties directly\n",
    "    config = {\n",
    "        'environment': vertex_ai_config.environment,\n",
    "        'project_id': vertex_ai_config.project_id,\n",
    "        'region': vertex_ai_config.region,\n",
    "        'artifact_registry_repo': vertex_ai_config.environment + '-' + vertex_ai_config.endpoint_name + '-repo',\n",
    "        'artifacts_bucket': vertex_ai_config.project_id + '-' + vertex_ai_config.environment + '-' + vertex_ai_config.endpoint_name + '-artifacts',\n",
    "        'endpoint_name': vertex_ai_config.endpoint_name,\n",
    "        **model_config  # Add model-specific config\n",
    "    }\n",
    "    \n",
    "    print(\"Configuration loaded successfully!\")\n",
    "    print(\"\\nEnvironment settings:\")\n",
    "    print(json.dumps(config, indent=2))\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease set the required environment variables before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Package Model\n",
    "Download the model from Hugging Face and package it for Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/steffen/.cache/huggingface/token\n",
      "Login successful\n",
      "Downloading model openai/whisper-small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 23:52:01.587346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 23:52:04.389221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ./types/huggingface/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steffen/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:388: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tar.gz archive...\n",
      "Uploading to gs://sign-lang-translator-20241029-dev-vertex-ai-artifacts/v1/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from google.cloud import storage\n",
    "\n",
    "def download_and_package_model(model_id: str, model_dir: str = './types/huggingface/model', skip_download: bool = False):\n",
    "    \"\"\"Download and package model, or use existing tar.gz if available\"\"\"\n",
    "    tar_path = './types/huggingface/model.tar.gz'\n",
    "    \n",
    "    # If tar.gz exists and skip_download is False, use existing file\n",
    "    if os.path.exists(tar_path) and not skip_download:\n",
    "        print(f\"Using existing {tar_path}\")\n",
    "        return tar_path\n",
    "    \n",
    "    # Otherwise, download and package the model\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading model {model_id}...\")\n",
    "    model = AutoModel.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    print(f\"Saving to {model_dir}\")\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    \n",
    "    print(\"Creating tar.gz archive...\")\n",
    "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "        tar.add(model_dir, arcname=\".\")\n",
    "    \n",
    "    return tar_path\n",
    "\n",
    "\n",
    "def upload_to_gcs(file_path: str, bucket_name: str, model_version: str):\n",
    "    from google.api_core import retry\n",
    "    destination_blob_name = f\"{model_version}/model.tar.gz\"\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    print(f\"Uploading to gs://{bucket_name}/{destination_blob_name}\")\n",
    "    \n",
    "    # Configure retry with longer timeout\n",
    "    retry_config = retry.Retry(\n",
    "        initial=1.0,  # Initial delay in seconds\n",
    "        maximum=60.0,  # Maximum delay between retries\n",
    "        multiplier=2.0,  # Multiplier applied to delay between retries\n",
    "        deadline=600.0  # Total timeout in seconds (10 minutes)\n",
    "    )\n",
    "    \n",
    "    blob.upload_from_filename(\n",
    "        file_path,\n",
    "        retry=retry_config,\n",
    "        timeout=600  # 10 minute timeout\n",
    "    )\n",
    "    \n",
    "    return f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "\n",
    "# Execute\n",
    "if 'HUGGINGFACE_TOKEN' in os.environ:\n",
    "    login(os.environ['HUGGINGFACE_TOKEN'])\n",
    "\n",
    "tar_path = download_and_package_model(config['model_id'], skip_download=True)\n",
    "artifacts_uri = upload_to_gcs(\n",
    "    tar_path, \n",
    "    config['artifacts_bucket'],\n",
    "    config['model_version']\n",
    ")\n",
    "print(f\"\\nModel artifacts uploaded to: {artifacts_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build and Push Docker Container\n",
    "Build the custom prediction routine container and push it to Artifact Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def build_and_push_image(config):\n",
    "    image_uri = f\"{config['region']}-docker.pkg.dev/{config['project_id']}/{config['artifact_registry_repo']}/{config['image_name']}:latest\"\n",
    "    \n",
    "    # Get paths relative to the project root\n",
    "    docker_path = project_root / \"models\" / \"huggingface_model\" / \"docker\"\n",
    "    \n",
    "    # Build command\n",
    "    build_cmd = [\n",
    "        \"docker\", \"build\",\n",
    "        \"-t\", image_uri,\n",
    "        \"--build-arg\", f\"HF_TASK={config['hf_task']}\",\n",
    "        \"--platform=linux/amd64\",\n",
    "        \"-f\", str(docker_path / \"Dockerfile\"),\n",
    "        str(docker_path)\n",
    "    ]\n",
    "    \n",
    "    print(\"Building Docker image...\")\n",
    "    subprocess.run(build_cmd, check=True)\n",
    "    \n",
    "    print(\"\\nPushing Docker image...\")\n",
    "    subprocess.run([\"docker\", \"push\", image_uri], check=True)\n",
    "    \n",
    "    return image_uri\n",
    "\n",
    "# Execute\n",
    "container_image_uri = build_and_push_image(config)\n",
    "print(f\"\\nContainer image available at: {container_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy to Vertex AI\n",
    "Finally, deploy the model to a Vertex AI endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "def deploy_model(config, artifacts_uri, container_image_uri):\n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(project=config['project_id'], location=config['region'])\n",
    "    \n",
    "    print(\"Uploading model to Vertex AI...\")\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=f\"hf-{config['model_id'].split('/')[-1]}\",\n",
    "        artifact_uri=artifacts_uri,\n",
    "        serving_container_image_uri=container_image_uri,\n",
    "        serving_container_environment_variables={\n",
    "            \"HF_TASK\": config['hf_task'],\n",
    "            \"VERTEX_CPR_WEB_CONCURRENCY\": \"1\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDeploying model to endpoint...\")\n",
    "    endpoint = model.deploy(\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "        endpoint_name=config['endpoint_name']\n",
    "    )\n",
    "    \n",
    "    return endpoint\n",
    "\n",
    "# Execute\n",
    "endpoint = deploy_model(config, artifacts_uri, container_image_uri)\n",
    "print(f\"\\nModel deployed successfully!\")\n",
    "print(f\"Endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Endpoint\n",
    "Let's test the deployed model with a sample prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from google.cloud import aiplatform_v1\n",
    "from google.api import httpbody_pb2\n",
    "\n",
    "def test_prediction(project_id: str, location: str, endpoint_id: str, test_data: dict):\n",
    "    client = aiplatform_v1.PredictionServiceClient(\n",
    "        client_options={\"api_endpoint\": f\"{location}-aiplatform.googleapis.com\"}\n",
    "    )\n",
    "    \n",
    "    endpoint = f\"projects/{project_id}/locations/{location}/endpoints/{endpoint_id}\"\n",
    "    \n",
    "    json_data = json.dumps(test_data)\n",
    "    http_body = httpbody_pb2.HttpBody(\n",
    "        data=json_data.encode(\"utf-8\"),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    "    \n",
    "    request = aiplatform_v1.RawPredictRequest(\n",
    "        endpoint=endpoint,\n",
    "        http_body=http_body,\n",
    "    )\n",
    "    \n",
    "    response = client.raw_predict(request)\n",
    "    return json.loads(response.data)\n",
    "\n",
    "# Test data for zero-shot classification\n",
    "test_data = {\n",
    "    \"sequences\": \"I need help with my account login\",\n",
    "    \"candidate_labels\": [\"account access\", \"billing\", \"technical issue\", \"general inquiry\"]\n",
    "}\n",
    "\n",
    "# Get endpoint ID from the endpoint resource name\n",
    "endpoint_id = endpoint.resource_name.split(\"/\")[-1]\n",
    "\n",
    "# Run test prediction\n",
    "result = test_prediction(\n",
    "    config['project_id'],\n",
    "    config['region'],\n",
    "    endpoint_id,\n",
    "    test_data\n",
    ")\n",
    "\n",
    "print(\"Prediction result:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
