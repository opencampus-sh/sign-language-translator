{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-cloud-storage\n",
    "\n",
    "# Authenticate\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Imports\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to save a pandas DataFrame as a parquet file to the data bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install google-cloud-storage pandas pyarrow\n",
    "\n",
    "# Authenticate\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Imports\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "def save_parquet_to_bucket(\n",
    "    df: pd.DataFrame, \n",
    "    project_id: str, \n",
    "    destination_path: str, \n",
    "    row_group_size: int = 750,\n",
    "    environment: str = \"dev\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame as a parquet file with specified row group size\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame to save\n",
    "        project_id: your GCP project ID\n",
    "        destination_path: path within the bucket (e.g., 'raw-data/myfile.parquet')\n",
    "        row_group_size: number of rows per group in parquet file (default: 750)\n",
    "        environment: environment name (default: \"dev\")\n",
    "    \"\"\"\n",
    "    # Initialize storage client\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket_name = f\"{project_id}-{environment}-data\"\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    # Create a blob reference\n",
    "    blob = bucket.blob(destination_path)\n",
    "    \n",
    "    # Save DataFrame to parquet in memory with specified row group size\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df.to_parquet(\n",
    "        parquet_buffer,\n",
    "        row_group_size=row_group_size,\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "    \n",
    "    # Upload to bucket\n",
    "    parquet_buffer.seek(0)\n",
    "    blob.upload_from_string(\n",
    "        parquet_buffer.getvalue(),\n",
    "        content_type='application/octet-stream'\n",
    "    )\n",
    "    \n",
    "    print(f\"DataFrame saved as parquet to gs://{bucket_name}/{destination_path}\")\n",
    "    print(f\"Row group size: {row_group_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'column1': range(2000),\n",
    "    'column2': [f'value_{i}' for i in range(2000)]\n",
    "})\n",
    "\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your project ID\n",
    "\n",
    "# Save to bucket with row group size of 750\n",
    "save_parquet_to_bucket(\n",
    "    df=df,\n",
    "    project_id=PROJECT_ID,\n",
    "    destination_path=\"raw-data/my_data.parquet\",\n",
    "    row_group_size=750\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(bucket, prefix: str = None):\n",
    "    \"\"\"List files in the bucket/prefix\"\"\"\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        print(f\"- {blob.name} ({blob.size} bytes)\")\n",
    "\n",
    "# List all files in raw-data/\n",
    "list_files(bucket, prefix='raw-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(source_path: str, bucket) -> pd.DataFrame:\n",
    "    \"\"\"Read a CSV file from cloud storage into a DataFrame\"\"\"\n",
    "    blob = bucket.blob(source_path)\n",
    "    content = blob.download_as_string()\n",
    "    return pd.read_csv(io.BytesIO(content))\n",
    "\n",
    "# Usage example:\n",
    "df = read_dataframe('raw-data/my_data.csv', bucket)\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
